# Step 6: Data Transformation & Storage Configuration

# Input data from Step 5 (paths relative to 06_Data_Transformation_Storage directory)
input_data:
  # Directory containing Step 5 outputs
  step5_output_dir: "../05_Data_Preparation/reports"
  # Leave null to auto-detect latest cleaned data file
  latest_cleaned_data: null
  # Leave null to auto-detect latest metadata file  
  metadata_file: null
  # Key columns that should not be transformed
  id_columns: ["CustomerId", "CustomerNum"]

# Feature engineering configuration
feature_engineering:
  # Customer-level aggregated features
  customer_aggregations:
    # Numerical aggregations
    numerical_features:
      - feature: "Balance"
        aggregations: ["sum", "mean", "std", "min", "max"]
      - feature: "CreditScore"
        aggregations: ["mean", "std"]
      - feature: "EstimatedSalary"
        aggregations: ["mean", "std"]
      - feature: "Age"
        aggregations: ["mean"]
    
    # Categorical aggregations
    categorical_features:
      - feature: "Geography"
        aggregations: ["mode", "count_unique"]
      - feature: "Gender" 
        aggregations: ["mode"]

  # Derived features to create
  derived_features:
    # Customer tenure (assuming account opening date available)
    - name: "customer_tenure_days"
      description: "Days since customer account creation"
      formula: "current_date - account_creation_date"
      
    # Balance to salary ratio
    - name: "balance_to_salary_ratio"
      description: "Customer balance as proportion of estimated salary"
      formula: "Balance / EstimatedSalary"
      
    # Credit score category
    - name: "credit_score_category"
      description: "Categorical credit score ranges"
      formula: "bin_credit_score"
      bins: [300, 580, 670, 740, 800, 850]
      labels: ["Poor", "Fair", "Good", "Very Good", "Excellent"]
      
    # Age group
    - name: "age_group"
      description: "Customer age categories"
      formula: "bin_age"
      bins: [0, 30, 45, 60, 100]
      labels: ["Young", "Middle-aged", "Senior", "Elderly"]
      
    # High value customer indicator
    - name: "high_value_customer"
      description: "Customer with balance > 75th percentile"
      formula: "Balance > balance_p75"
      
    # Geographic risk score
    - name: "geographic_risk_score"
      description: "Risk score based on geography"
      formula: "geography_risk_mapping"

  # Temporal features (if timestamp columns available)
  temporal_features:
    enabled: true
    # Account activity patterns
    activity_patterns:
      - monthly_activity
      - seasonal_patterns
      - account_age_buckets
    
    # Time-based aggregations
    time_windows: [30, 90, 180, 365]  # days

  # Feature scaling and normalization
  scaling:
    method: "standard"  # standard, minmax, robust, none
    # Features to exclude from scaling
    exclude_columns: 
      - "CustomerId"
      - "CustomerNum" 
      - "Exited"
      - "credit_score_category"
      - "age_group"
      - "high_value_customer"

# Database configuration
database:
  # SQLite database path (relative to 06_Data_Transformation_Storage)
  db_path: "datawarehouse/customer_data_warehouse.db"
  # SQL schema file
  schema_file: "sql/schema.sql"
  # Enable database backups
  backup_enabled: true
  backup_path: "datawarehouse/backups"
  
  # Table configurations
  tables:
    # Dimension table for customers
    dim_customers:
      primary_key: "customer_id"
      indexes: ["geography", "gender", "age_group"]
      
    # Fact table for customer features
    fact_customer_features:
      primary_key: "feature_id"
      foreign_keys:
        - column: "customer_id"
          references: "dim_customers(customer_id)"
      indexes: ["customer_id", "last_updated"]
      
    # Metadata table
    transformation_metadata:
      primary_key: "transformation_id"
      indexes: ["transformation_date"]

# Output configuration
output:
  # Output directory for reports and transformed data
  reports_dir: "reports"
  # Logs directory
  logs_dir: "logs"
  # Export formats for transformed data
  export_formats: ["csv", "parquet", "json"]
  
  # Output file naming
  transformed_data_prefix: "transformed_data"
  feature_summary_prefix: "feature_engineering_summary"
  
  # Reports to generate
  reports:
    - feature_distribution_analysis
    - correlation_analysis
    - transformation_summary
    - data_quality_metrics

# Processing configuration
processing:
  # Batch size for large datasets
  batch_size: 10000
  # Memory optimization
  optimize_memory: true
  # Parallel processing
  n_jobs: -1  # Use all available cores
  
  # Data validation settings
  validation:
    # Validate input data quality
    input_validation: true
    # Validate feature creation
    feature_validation: true
    # Validate database integrity
    database_validation: true
    
    # Quality thresholds
    quality_thresholds:
      min_completeness: 0.95  # 95% non-null values
      max_duplicate_rate: 0.01  # 1% duplicates allowed
      min_feature_variance: 0.001  # Minimum feature variance

# Logging configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(levelname)s - %(name)s - %(message)s"
  
  # Separate log files
  log_files:
    transformation: "data_transformation.log"
    database: "database_operations.log"
    features: "feature_engineering.log"
    errors: "errors.log"

# Integration settings
integration:
  # Upstream dependencies
  upstream:
    step5_required: true
    validate_step5_outputs: true
    
  # Downstream preparation
  downstream:
    prepare_for_step7: true
    feature_selection_ready: true
    model_ready_format: true